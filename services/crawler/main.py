#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºAPIç½‘ç»œè¯·æ±‚çš„æ•°æ®æå–è„šæœ¬
ä½¿ç”¨Playwrightæ‹¦æˆª/UIProcessorè¯·æ±‚è·å–æ•°æ®
"""

import asyncio
import logging
import os
import sys
import argparse
from datetime import datetime
from crawler import KSXCrawler


async def main(target_date: str = None):
    """ä¸»å‡½æ•° - æ‰§è¡ŒåŸºäºAPIçš„æ•°æ®æå–"""
    if target_date:
        print(f"ğŸš€ å¼€å§‹åŸºäºAPIçš„KSXæ•°æ®æå–ï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")
    else:
        print("ğŸš€ å¼€å§‹åŸºäºAPIçš„KSXæ•°æ®æå–...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰
    from config import get_config
    config = get_config()
    crawler = KSXCrawler(headless=config['browser']['headless'], timeout=30000)
    
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸï¼Œè®¾ç½®çˆ¬è™«çš„ç›®æ ‡æ—¥æœŸ
    if target_date:
        crawler.target_date = target_date
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        print("ğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        browser_success = await crawler.start_browser()
        if not browser_success:
            print("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
            print("ERROR_TYPE: BROWSER_START_FAILED")
            return
        
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        
        # æ‰§è¡Œç™»å½•
        print("ğŸ” æ­£åœ¨ç™»å½•...")
        login_success = await crawler.login()
        if not login_success:
            print("âŒ ç™»å½•å¤±è´¥")
            print("ERROR_TYPE: LOGIN_FAILED")
            return
        
        print("âœ… ç™»å½•æˆåŠŸ")
        
        # æ‰§è¡Œå®Œæ•´çš„APIæ•°æ®æå–æµç¨‹
        print("ğŸ“Š å¼€å§‹APIæ•°æ®æå–...")
        extraction_success = await crawler.full_api_data_extraction()
        
        if extraction_success:
            print("ğŸ‰ APIæ•°æ®æå–å®Œæˆï¼")
        else:
            print("âŒ APIæ•°æ®æå–å¤±è´¥")
            print("ERROR_TYPE: DATA_EXTRACTION_FAILED")
        
        # è‡ªåŠ¨å…³é—­ï¼Œæ— éœ€ç”¨æˆ·ç¡®è®¤
        print("ğŸ”„ ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨è‡ªåŠ¨å…³é—­...")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        print("ERROR_TYPE: USER_INTERRUPTED")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        print("ERROR_TYPE: UNKNOWN_ERROR")
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
    finally:
        # æ¸…ç†èµ„æº
        print("ğŸ”„ æ­£åœ¨æ¸…ç†èµ„æº...")
        await crawler.close()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='KSXæ•°æ®çˆ¬è™«')
    parser.add_argument('--date', type=str, help='æŒ‡å®šè¦çˆ¬å–çš„æ—¥æœŸ (YYYY-MM-DD)')
    args = parser.parse_args()
    
    # è®¾ç½®åŸºæœ¬æ—¥å¿—é…ç½®
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('api_extraction.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main(args.date))
